# file: video_action_infer.py
import os
from typing import List, Tuple

import cv2
import numpy as np
import torch
import torch.nn as nn
from torchvision import models, transforms

from dataset_preprocess import SEQ_LEN, IMG_SIZE, ACTIONS, eval_augment

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BASE_PATH = r"C:/ai_project01/ucf101_top5"
BEST_CKPT = os.path.join(BASE_PATH, "best_resnet18_lstm.pth")  # 05.04, 05.05와 동일 경로 가정 :contentReference[oaicite:5]{index=5}

# PyTorch 전용 추가 Normalize (Albumentations ToTensorV2 대신 쓰는 경우 대비) :contentReference[oaicite:6]{index=6}
pt_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])


class FrameEncoderResNet18(nn.Module):
    """단일 프레임을 512차원 feature로 인코딩."""

    def __init__(self):
        super().__init__()
        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        # 마지막 FC 제거
        modules = list(backbone.children())[:-1]  # [B, 512, 1, 1]
        self.backbone = nn.Sequential(*modules)

    def forward(self, x):
        # x: [B*T, 3, H, W]
        feats = self.backbone(x)          # [B*T, 512, 1, 1]
        feats = feats.flatten(1)          # [B*T, 512]
        return feats


class ResNet18LSTM(nn.Module):
    """ResNet18(프레임 인코더) + LSTM(시퀀스 모델) """

    def __init__(self, num_classes: int, hidden_size: int = 512, num_layers: int = 1,
                 dropout: float = 0.3):
        super().__init__()
        self.encoder = FrameEncoderResNet18()
        self.lstm = nn.LSTM(
            input_size=512,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
        )
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        # x: [B, T, C, H, W]
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        feats = self.encoder(x)         # [B*T, 512]
        feats = feats.view(B, T, -1)    # [B, T, 512]
        out, _ = self.lstm(feats)       # [B, T, H]
        last = out[:, -1, :]            # 마지막 타임스텝 사용
        logits = self.fc(last)          # [B, num_classes]
        return logits


def read_uniform_frames(video_path: str, seq_len: int = SEQ_LEN) -> List[np.ndarray]:
    """05.04 설명과 동일한 균등간격 프레임 추출. :contentReference[oaicite:8]{index=8}"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video: {video_path}")

    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    total = max(total, seq_len)
    indices = np.linspace(0, total - 1, seq_len).astype(int)

    frames = []
    last_valid = None
    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))
        ok, frame = cap.read()
        if not ok:
            frame = last_valid if last_valid is not None \
                else np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)
        else:
            last_valid = frame
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame)

    cap.release()
    return frames


def load_model(ckpt_path: str = BEST_CKPT) -> ResNet18LSTM:
    model = ResNet18LSTM(num_classes=len(ACTIONS))
    state = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(state)
    model.to(device)
    model.eval()
    return model


@torch.no_grad()
def predict_video(model: nn.Module, video_path: str, topk: int = 3) -> List[Tuple[str, float]]:
    frames = read_uniform_frames(video_path, SEQ_LEN)
    # Albumentations eval_augment 사용
    tens = [eval_augment(image=f)["image"] for f in frames]  # [T, C, H, W] tensor
    clip = torch.stack(tens, dim=0).unsqueeze(0).to(device)  # [1, T, C, H, W]

    logits = model(clip)                     # [1, num_classes]
    probs = torch.softmax(logits, dim=1)[0]  # [num_classes]

    values, indices = probs.topk(topk)
    results = []
    for v, idx in zip(values.tolist(), indices.tolist()):
        results.append((ACTIONS[idx], float(v)))
    return results


def ask_and_predict():
    import tkinter as tk
    from tkinter import filedialog

    root = tk.Tk()
    root.withdraw()
    path = filedialog.askopenfilename(
        title="Choose a video",
        filetypes=[("Video", "*.mp4;*.avi;*.mov"), ("All files", "*.*")]
    )
    if not path:
        print("Canceled")
        return

    model = load_model()
    topk = predict_video(model, path, topk=3)

    print(f"File: {path}")
    for i, (name, prob) in enumerate(topk, 1):
        print(f"Top-{i}: {name} (prob={prob:.3f})")


if __name__ == "__main__":
    ask_and_predict()
