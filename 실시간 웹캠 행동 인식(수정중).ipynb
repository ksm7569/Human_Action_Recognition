# file: realtime_action.py
import os
from collections import deque
from typing import Optional, Tuple

import cv2
import numpy as np
import torch
import torch.nn as nn
from ultralytics import YOLO

from dataset_preprocess import SEQ_LEN, IMG_SIZE, ACTIONS, eval_augment
from video_action_infer import ResNet18LSTM, BEST_CKPT, device

# -----------------------------
# 하이퍼파라미터 (05.05 설명 기반) :contentReference[oaicite:10]{index=10}
# -----------------------------
det_conf = 0.30       # 사람 탐지 confidence threshold
det_iou = 0.45        # NMS IoU threshold
expand_ratio = 0.30   # 사람 박스를 주변으로 확장
detect_every = 2      # 매 N프레임마다 YOLO 재탐지
keep_on_lost = 8      # 사람 안 보일 때 유지 프레임 수

momentum = 0.6        # bbox smoothing
topk = 3              # 상위 행동 개수
alpha = 0.25          # 확률 EMA 가중치 :contentReference[oaicite:11]{index=11}


# -----------------------------
# 보조 함수들 (_clip_xyxy, _expand_box, detect_person_box, smooth_box 등)
# -----------------------------
def _clip_xyxy(x1, y1, x2, y2, w, h):
    """좌표가 이미지 밖으로 나가지 않도록 클리핑. :contentReference[oaicite:12]{index=12}"""
    x1 = max(0, min(int(x1), w - 1))
    y1 = max(0, min(int(y1), h - 1))
    x2 = max(0, min(int(x2), w - 1))
    y2 = max(0, min(int(y2), h - 1))
    return x1, y1, x2, y2


def _expand_box(x1, y1, x2, y2, w, h, expand=expand_ratio):
    """사람 박스를 가운데 기준으로 조금 더 키움. :contentReference[oaicite:13]{index=13}"""
    cx = (x1 + x2) / 2.0
    cy = (y1 + y2) / 2.0
    bw = (x2 - x1)
    bh = (y2 - y1)

    half_w = bw * (0.5 + expand)
    half_h = bh * (0.5 + expand)

    nx1 = cx - half_w
    ny1 = cy - half_h
    nx2 = cx + half_w
    ny2 = cy + half_h
    return _clip_xyxy(nx1, ny1, nx2, ny2, w, h)


def detect_person_box(
    yolo: YOLO,
    rgb_img: np.ndarray,
    conf: float = det_conf,
    iou: float = det_iou
) -> Optional[Tuple[int, int, int, int, float]]:
    """
    한 장의 RGB 이미지에서 가장 큰 'person' 박스를 1개만 반환.
    반환: (x1, y1, x2, y2, score) 또는 None.
    """
    h, w, _ = rgb_img.shape
    results = yolo.predict(rgb_img, conf=conf, iou=iou, verbose=False)
    if not results:
        return None
    res = results[0]
    if res.boxes is None or len(res.boxes) == 0:
        return None

    best = None
    best_area = 0.0
    for b in res.boxes:
        cls_id = int(b.cls.item())
        if res.names[cls_id] != "person":
            continue
        score = float(b.conf.item())
        x1, y1, x2, y2 = b.xyxy[0].cpu().numpy().tolist()
        x1, y1, x2, y2 = _clip_xyxy(x1, y1, x2, y2, w, h)
        area = (x2 - x1) * (y2 - y1)
        if area > best_area:
            best_area = area
            best = (x1, y1, x2, y2, score)

    return best


def smooth_box(last_box, new_box, momentum=momentum):
    """이전 박스와 새 박스를 EMA로 부드럽게 섞기. :contentReference[oaicite:14]{index=14}"""
    if last_box is None:
        return new_box
    lx1, ly1, lx2, ly2 = last_box
    nx1, ny1, nx2, ny2 = new_box
    x1 = lx1 * momentum + nx1 * (1 - momentum)
    y1 = ly1 * momentum + ny1 * (1 - momentum)
    x2 = lx2 * momentum + nx2 * (1 - momentum)
    y2 = ly2 * momentum + ny2 * (1 - momentum)
    return (int(x1), int(y1), int(x2), int(y2))


def smooth_probs(ema: Optional[torch.Tensor], probs: torch.Tensor, alpha=alpha):
    """이전 확률과 현재 확률을 EMA로 부드럽게 합침. :contentReference[oaicite:15]{index=15}"""
    if ema is None:
        return probs
    return ema * (1 - alpha) + probs * alpha


def draw_result(frame, draw_box, conf, ema_probs, actions, topk=3):
    h, w, _ = frame.shape
    overlay = frame.copy()

    if draw_box is not None:
        x1, y1, x2, y2 = draw_box
        cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(
            overlay,
            f"person {conf:.2f}",
            (x1, max(0, y1 - 10)),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            (0, 255, 0),
            2,
        )
    else:
        # 사람 못 찾으면 화면 전체를 주황색 tint :contentReference[oaicite:16]{index=16}
        overlay[:] = (0, 140, 255)

    # 상위 top-k 행동 막대 그래프
    if ema_probs is not None:
        probs = ema_probs.cpu()
        vals, idxs = probs.topk(topk)
        # 그래프 영역
        bar_w = int(w * 0.5)
        bar_h = 18
        start_x = 10
        start_y = 10

        for i, (v, idx) in enumerate(zip(vals.tolist(), idxs.tolist())):
            y = start_y + i * (bar_h + 8)
            p = float(v)
            text = f"{i+1}. {actions[idx]} ({p:.2f})"
            cv2.putText(
                overlay,
                text,
                (start_x, y + 14),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (0, 0, 0),
                2,
            )
            cv2.rectangle(
                overlay,
                (start_x, y + 18),
                (start_x + int(bar_w * p), y + 18 + bar_h),
                (255, 255, 255),
                -1,
            )

    return overlay


# -----------------------------
# 모델 로딩
# -----------------------------
def load_models():
    yolo = YOLO("yolov8n.pt")  # 사람 검출용
    action_model = ResNet18LSTM(num_classes=len(ACTIONS))
    state = torch.load(BEST_CKPT, map_location=device)
    action_model.load_state_dict(state)
    action_model.to(device)
    action_model.eval()
    return yolo, action_model


@torch.no_grad()
def main():
    yolo, model = load_models()

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        raise RuntimeError("Cannot open webcam")

    buf = deque(maxlen=SEQ_LEN)
    last_box = None
    last_conf = 0.0
    lost_count = 0
    ema = None
    frame_idx = 0

    while True:
        ok, frame = cap.read()
        if not ok:
            break

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, _ = rgb.shape

        # 1) 사람 탐지 (매 detect_every 프레임마다 YOLO 호출) :contentReference[oaicite:17]{index=17}
        if frame_idx % detect_every == 0:
            box_info = detect_person_box(yolo, rgb, det_conf, det_iou)
            if box_info is not None:
                x1, y1, x2, y2, conf = box_info
                x1, y1, x2, y2 = _expand_box(x1, y1, x2, y2, w, h, expand_ratio)
                last_box = (x1, y1, x2, y2)
                last_conf = conf
                lost_count = 0
            else:
                lost_count += 1
                if lost_count > keep_on_lost:
                    last_box = None
                    last_conf = 0.0
        frame_idx += 1

        # 2) ROI 자르고 전처리 → 버퍼에 push :contentReference[oaicite:18]{index=18}
        if last_box is not None:
            x1, y1, x2, y2 = last_box
            roi = rgb[y1:y2, x1:x2]
            aug = eval_augment(image=roi)
            ten = aug["image"]  # [C,H,W] tensor
            buf.append(ten)
            draw_box = last_box
        else:
            draw_box = None

        # 3) 버퍼가 꽉 차면 행동 예측 :contentReference[oaicite:19]{index=19}
        if len(buf) == SEQ_LEN:
            clip = torch.stack(list(buf), dim=0).unsqueeze(0).to(device)
            logits = model(clip)
            probs = torch.softmax(logits, dim=1)[0]
            ema = smooth_probs(ema, probs, alpha)

        # 4) 화면에 결과 그리기
        vis = draw_result(frame, draw_box, last_conf, ema, ACTIONS, topk=topk)

        cv2.imshow("Real-time Action Recognition", vis)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
