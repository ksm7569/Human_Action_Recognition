# file: dataset_preprocess.py
import os
from typing import List, Tuple

import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2

# -----------------------------
# 공통 설정
# -----------------------------
SEQ_LEN = 20          # 한 동영상에서 사용할 프레임 수 :contentReference[oaicite:1]{index=1}
IMG_SIZE = 224        # 최종 입력 크기

ACTIONS = ['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']
NUM_CLASSES = len(ACTIONS)

# -----------------------------
# 이미지 증강 (train용) – ReplayCompose 사용 :contentReference[oaicite:2]{index=2}
# -----------------------------
train_augment = A.ReplayCompose([
    # 1) 랜덤 리사이즈 크롭
    A.RandomResizedCrop(
        height=IMG_SIZE,
        width=IMG_SIZE,
        scale=(0.6, 1.0),
        ratio=(0.85, 1.33),
        p=0.7,
    ),
    # 2) 좌우 반전
    A.HorizontalFlip(p=0.5),
    # 3) 밝기/대비/색상 등
    A.OneOf([
        A.RandomBrightnessContrast(
            brightness_limit=0.4,
            contrast_limit=0.4,
            p=1.0
        ),
        A.HueSaturationValue(
            hue_shift_limit=8,
            sat_shift_limit=20,
            val_shift_limit=20,
            p=1.0
        ),
        A.RGBShift(
            r_shift_limit=10,
            g_shift_limit=10,
            b_shift_limit=10,
            p=1.0
        ),
        A.RandomGamma(gamma_limit=(80, 120), p=1.0),
        A.ToGray(p=1.0),
    ], p=0.8),
    # 4) Blur
    A.OneOf([
        A.MotionBlur(blur_limit=7, p=1.0),
        A.GaussianBlur(blur_limit=(3, 5), p=1.0),
        A.MedianBlur(blur_limit=5, p=1.0),
    ], p=0.5),
    # 5) Noise / Compression
    A.OneOf([
        A.ISONoise(p=1.0),
        A.GaussNoise(p=1.0),
        A.JpegCompression(quality_lower=60, quality_upper=95, p=1.0),
        A.Downscale(scale_min=0.85, scale_max=0.95, p=1.0),
    ], p=0.5),
    # 6) 기하 변형
    A.Affine(
        rotate=(-15, 15),
        translate_percent=(0.0, 0.12),
        scale=(0.85, 1.15),
        shear=(-7, 7),
        fit_output=True,
        p=0.7,
    ),
    # 7) 퍼스펙티브
    A.Perspective(scale=(0.02, 0.08), p=0.3),
    # 8) CoarseDropout
    A.CoarseDropout(
        max_holes=IMG_SIZE // 10,
        max_height=IMG_SIZE // 10,
        max_width=IMG_SIZE // 10,
        p=0.5,
    ),
    # 9) 최종 리사이즈 + 텐서 변환
    A.Resize(IMG_SIZE, IMG_SIZE),
    ToTensorV2()
])

# -----------------------------
# 평가용 증강 (05.04/05.05의 eval_augment와 호환) 
# -----------------------------
eval_augment = A.Compose([
    A.LongestMaxSize(max_size=IMG_SIZE),
    A.PadIfNeeded(
        min_height=IMG_SIZE,
        min_width=IMG_SIZE,
        border_mode=cv2.BORDER_CONSTANT,
        value=(0, 0, 0),
    ),
    A.Resize(IMG_SIZE, IMG_SIZE),
    ToTensorV2()
])


def read_uniform_frames(video_path: str, seq_len: int = SEQ_LEN) -> List[np.ndarray]:
    """동영상에서 seq_len 장을 균등 간격으로 샘플링."""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video: {video_path}")

    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    total = max(total, seq_len)
    indices = np.linspace(0, total - 1, seq_len).astype(int)

    frames = []
    last_valid = None
    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))
        ok, frame = cap.read()
        if not ok:
            frame = last_valid if last_valid is not None \
                else np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)
        else:
            last_valid = frame
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame)

    cap.release()
    return frames


class ActionDataset(Dataset):
    """
    동영상 경로와 레이블이 담긴 txt/CSV를 읽어오는 Dataset.
    한 줄 형식: <video_path> <class_name>
    """

    def __init__(self, list_file: str, train: bool = True):
        self.items: List[Tuple[str, int]] = []
        with open(list_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                path, cls = line.split()
                label = ACTIONS.index(cls)
                self.items.append((path, label))
        self.train = train

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        video_path, label = self.items[idx]
        frames = read_uniform_frames(video_path, SEQ_LEN)

        if self.train:
            # 첫 프레임에 대해 증강을 실행하고, Replay를 나머지 프레임에 재적용
            first = frames[0]
            auged = train_augment(image=first)
            aug_frames = [auged["image"]]
            replay = auged["replay"]

            for frm in frames[1:]:
                aug = A.ReplayCompose.replay(replay, image=frm)
                aug_frames.append(aug["image"])
        else:
            aug_frames = [eval_augment(image=frm)["image"] for frm in frames]

        # [T, C, H, W]
        clip = torch.stack(auged if self.train else aug_frames, dim=0)
        label = torch.tensor(label, dtype=torch.long)
        return clip, label


if __name__ == "__main__":
    # 간단 동작 테스트
    ds = ActionDataset("train_list.txt", train=True)
    dl = DataLoader(ds, batch_size=2, shuffle=True)
    x, y = next(iter(dl))
    print(x.shape, y.shape)  # torch.Size([2, 20, 3, 224, 224]) 등
